{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6e67a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CustomPongEnv_v0_ram import Custom_Pong\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(10)\n",
    "np.random.seed(10)\n",
    "\n",
    "env = Custom_Pong()\n",
    "states=env.observation_space.shape[0]\n",
    "actions=env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c37e40be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_obs(state, reward):\n",
    "    \n",
    "    factor = 5\n",
    "    \n",
    "    ball_x = state[0]\n",
    "    o1_float = ball_x/factor\n",
    "    o1 = int(o1_float)\n",
    "\n",
    "    ball_y = state[1] \n",
    "    o2_float = ball_y/factor\n",
    "    o2 = int(o2_float)\n",
    "\n",
    "    ball_vx = state[2]\n",
    "    o3_float = ball_vx\n",
    "    o3_i = int(o3_float)\n",
    "    o3 = 0 if o3_i==2 else 1 \n",
    "\n",
    "\n",
    "    ball_vy = state[3]\n",
    "    o4_float = ball_vx\n",
    "    o4_i = int(o4_float)\n",
    "    o4 = 0 if o4_i==2 else 1 \n",
    "\n",
    "    paddle_pos = state[4]\n",
    "    o5_float = paddle_pos/factor\n",
    "    o5 = int(o5_float)\n",
    "\n",
    "    paddle_vel = state[5]\n",
    "    o6_float = paddle_vel\n",
    "    o6_i = int(o6_float)\n",
    "    o6 = 0 if o6_i==4 else 1 \n",
    "    \n",
    "    if(reward == -1):\n",
    "        o7 = 0\n",
    "    elif(reward == 1):\n",
    "        o7 = 2\n",
    "    else:\n",
    "        o7 = 1\n",
    "    \n",
    "    observation = [o1, o2, o3, o4, o5, o6, o7]\n",
    "    \n",
    "    return(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733f3140",
   "metadata": {},
   "source": [
    "# Active Inference\n",
    "## Setting up states and factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b68abee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aswin\\anaconda3\\lib\\site-packages\\seaborn\\rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(mpl.__version__) >= \"3.0\":\n",
      "C:\\Users\\aswin\\anaconda3\\lib\\site-packages\\setuptools\\_distutils\\version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n"
     ]
    }
   ],
   "source": [
    "import pymdp\n",
    "from pymdp import utils\n",
    "import numpy as np\n",
    "from scipy.stats import dirichlet\n",
    "from pymdp.agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49cdbdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Hidden)Factors\n",
    "s1_size = 42\n",
    "\n",
    "num_states = [s1_size]\n",
    "num_factors = len(num_states)\n",
    "\n",
    "# Rewards\n",
    "reward_modes = 3 #Max score-5 (assumption)\n",
    "\n",
    "# Controls\n",
    "s1_actions = ['Stay', 'Play-Up', 'Play-Down']\n",
    "\n",
    "num_controls = [len(s1_actions)]\n",
    "\n",
    "# Observations\n",
    "o1_obs_size = 42\n",
    "o2_obs_size = 42\n",
    "o3_obs_size = 2\n",
    "o4_obs_size = 2\n",
    "o5_obs_size = 42\n",
    "o6_obs_size = 2\n",
    "reward_obs_size = reward_modes\n",
    "\n",
    "num_obs = [o1_obs_size, o2_obs_size, o3_obs_size, o4_obs_size, o5_obs_size, o6_obs_size, reward_obs_size]\n",
    "num_modalities = len(num_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33e7ac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS_VAL = 1e-16 # Negligibleconstant\n",
    "\n",
    "# Likelhiood Dynamics\n",
    "a = utils.random_A_matrix(num_obs, num_states) * 0 + EPS_VAL\n",
    "A = utils.random_A_matrix(num_obs, num_states)\n",
    "\n",
    "# Transisition dynamics\n",
    "# Initialised as random becuase the agent need to learn the dynamics\n",
    "\n",
    "B = utils.random_B_matrix(num_states, num_controls)\n",
    "b = utils.random_B_matrix(num_states, num_controls) * 0 + EPS_VAL\n",
    "B[0].shape\n",
    "\n",
    "# Normalising A and B as probability distributions\n",
    "\n",
    "numS = num_states[0]\n",
    "numA = num_controls[0]\n",
    "\n",
    "for i in range(numS):\n",
    "    A[0][:,i] = dirichlet.mean(a[0][:,i])\n",
    "    A[1][:,i] = dirichlet.mean(a[1][:,i])\n",
    "    A[2][:,i] = dirichlet.mean(a[2][:,i])\n",
    "    A[3][:,i] = dirichlet.mean(a[3][:,i])\n",
    "    A[4][:,i] = dirichlet.mean(a[4][:,i])\n",
    "    A[5][:,i] = dirichlet.mean(a[5][:,i])\n",
    "    A[6][:,i] = dirichlet.mean(a[6][:,i])\n",
    "    for j in range(numA):\n",
    "        B[0][:,i,j]=dirichlet.mean(b[0][:,i,j])\n",
    "\n",
    "# Prior preferences for biasing the generative model to control behaviour\n",
    "\n",
    "# The preferences are set uniform for all the hidden-states except the reward function\n",
    "C = utils.obj_array_uniform(num_obs)\n",
    "\n",
    "# Highest for the high-score and lowest for the lowscore\n",
    "C_score = np.array([-5.8, 0 , 1])\n",
    "# Normalising the prior preference\n",
    "C[6] = pymdp.maths.softmax(1*C_score)\n",
    "C[6]\n",
    "\n",
    "D = utils.obj_array_uniform([s1_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f7d9f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic programming active inference planner\n",
    "from ai_agent_planner import action_dist\n",
    "\n",
    "trials = 10\n",
    "episodes = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1fb9d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 297 ms\n",
      "Wall time: 297 ms\n"
     ]
    }
   ],
   "source": [
    "%time Q_pi = action_dist(A[6], B, C[6], T=20, sm_par=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96dd36fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI Agent-1 Planning horizon=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9b5aec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial 0\n",
      "trial 1\n",
      "trial 2\n",
      "trial 3\n",
      "trial 4\n",
      "trial 5\n",
      "trial 6\n",
      "trial 7\n",
      "trial 8\n",
      "trial 9\n",
      "CPU times: user 14h 36min 27s, sys: 40.9 s, total: 14h 37min 8s\n",
      "Wall time: 14h 36min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "planning_horizon = 5\n",
    "T = planning_horizon\n",
    "\n",
    "rally_length_raw = np.zeros((trials,episodes))\n",
    "\n",
    "for trial in range(trials):\n",
    "    print(\"trial\", trial)\n",
    "    \n",
    "    EPS_VAL = 1e-16 #negligibleconstant\n",
    "    # Likelhiood Dynamics\n",
    "    a = utils.random_A_matrix(num_obs, num_states) * 0 + EPS_VAL\n",
    "    A = utils.random_A_matrix(num_obs, num_states)\n",
    "\n",
    "    # Transisition dynamics\n",
    "    # Initialised as random becuase the agent need to learn the dynamics\n",
    "\n",
    "    B = utils.random_B_matrix(num_states, num_controls)\n",
    "    b = utils.random_B_matrix(num_states, num_controls) * 0 + EPS_VAL\n",
    "    B[0].shape\n",
    "\n",
    "    # Normalising A and B as probability distributions\n",
    "\n",
    "    numS = num_states[0]\n",
    "    numA = num_controls[0]\n",
    "\n",
    "    for i in range(numS):\n",
    "        A[0][:,i] = dirichlet.mean(a[0][:,i])\n",
    "        A[1][:,i] = dirichlet.mean(a[1][:,i])\n",
    "        A[2][:,i] = dirichlet.mean(a[2][:,i])\n",
    "        A[3][:,i] = dirichlet.mean(a[3][:,i])\n",
    "        A[4][:,i] = dirichlet.mean(a[4][:,i])\n",
    "        A[5][:,i] = dirichlet.mean(a[5][:,i])\n",
    "        A[6][:,i] = dirichlet.mean(a[6][:,i])\n",
    "        for j in range(numA):\n",
    "            B[0][:,i,j]=dirichlet.mean(b[0][:,i,j])\n",
    "\n",
    "    # Prior preferences for biasing the generative model to control behaviour\n",
    "\n",
    "    # The preferences are set uniform for all the hidden-states except the reward function\n",
    "    C = utils.obj_array_uniform(num_obs)\n",
    "\n",
    "    # Highest for the high-score and lowest for the lowscore\n",
    "    C_score = np.array([-5.8, 0 , 1])\n",
    "    # Normalising the prior preference\n",
    "    C[6] = pymdp.maths.softmax(1*C_score)\n",
    "    C[6]\n",
    "\n",
    "    D = utils.obj_array_uniform([s1_size])\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        # print(\"episode\", episode)\n",
    "\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        t=0\n",
    "        while not done:\n",
    "            # env.render()\n",
    "            # print(t)\n",
    "            if(t==0):\n",
    "                qs = D\n",
    "                qs_prev = qs\n",
    "                Q_pi = action_dist(A[6], B, C[6], T=planning_horizon, sm_par=1)\n",
    "\n",
    "            action = np.random.choice([0,1,2], size=None, replace=True, p=np.matmul(Q_pi[t],qs[0]))\n",
    "\n",
    "            state, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "\n",
    "            observation = state_to_obs(state, reward)\n",
    "\n",
    "            qs = pymdp.inference.update_posterior_states(A, observation, prior = D)\n",
    "            if(t == 0):\n",
    "                qs0 = qs\n",
    "\n",
    "            pA = A\n",
    "            pB = B\n",
    "            pD = D\n",
    "\n",
    "            # Learning\n",
    "            A = pymdp.learning.update_obs_likelihood_dirichlet(pA, A, observation, qs, lr=1.0, modalities='all')\n",
    "\n",
    "            actions = np.array([int(action)])\n",
    "            B = pymdp.learning.update_state_likelihood_dirichlet(pB, B, actions, qs, qs_prev, lr=1.0, factors='all')\n",
    "\n",
    "            if(t == 0):\n",
    "                D = pymdp.learning.update_state_prior_dirichlet(pD, qs0, lr=1.0, factors='all')\n",
    "                D[0] = pymdp.maths.softmax(D[0])\n",
    "\n",
    "            t += 1\n",
    "            qs_prev = qs\n",
    "\n",
    "            if(t == T-1):\n",
    "                t = 0\n",
    "                #Normalising A and B\n",
    "                for i in range(numS):\n",
    "                    A[0][:,i] = dirichlet.mean(A[0][:,i])\n",
    "                    A[1][:,i] = dirichlet.mean(A[1][:,i])\n",
    "                    A[2][:,i] = dirichlet.mean(A[2][:,i])\n",
    "                    A[3][:,i] = dirichlet.mean(A[3][:,i])\n",
    "                    A[4][:,i] = dirichlet.mean(A[4][:,i])\n",
    "                    A[5][:,i] = dirichlet.mean(A[5][:,i])\n",
    "                    A[6][:,i] = dirichlet.mean(A[6][:,i])\n",
    "                    for j in range(numA):\n",
    "                        B[0][:,i,j]=dirichlet.mean(B[0][:,i,j])\n",
    "\n",
    "        rally_length_raw[trial][episode] = score+1\n",
    "\n",
    "    env.close()\n",
    "    \n",
    "with open('rally_length_T5.npy', 'wb') as file:\n",
    "    np.save(file, rally_length_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813b6789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
