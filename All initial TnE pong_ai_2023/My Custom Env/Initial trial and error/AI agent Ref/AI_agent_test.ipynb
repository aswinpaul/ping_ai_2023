{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed63608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ale_py\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from math_functions import log_stable\n",
    "from math_functions import softmax\n",
    "from math_functions import onehot\n",
    "from math_functions import kl_div\n",
    "from math_functions import obj_array,obj_array_zeros\n",
    "\n",
    "from scipy.stats import dirichlet\n",
    "\n",
    "# Agent functions\n",
    "from ai_agent_planner import action_dist\n",
    "from infer_state import infer_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d41e27e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v0', render_mode='rgb_array')\n",
    "action_space=np.array([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d1880e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210, 160, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c0878e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67c63fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aswin\\anaconda3\\lib\\site-packages\\gym\\envs\\atari\\environment.py:255: UserWarning: \u001b[33mWARN: We strongly suggest supplying `render_mode` when constructing your environment, e.g., gym.make(ID, render_mode='human'). Using `render_mode` provides access to proper scaling, audio support, and proper framerates.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\aswin\\anaconda3\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4072/2608524792.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#             print(r)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i_episode in range(5):\n",
    "    observation = env.reset()\n",
    "    r=0\n",
    "    for t in range(1000):\n",
    "        env.render()    \n",
    "        n='name'\n",
    "        action = np.random.choice(action_space)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "#         print(action)\n",
    "        if(t==500):\n",
    "            ss=observation\n",
    "            env.env.ale.saveScreenPNG(b'test_image.png')\n",
    "#         print(reward)\n",
    "        r+=reward\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "#             print(r)\n",
    "            break\n",
    "        time.sleep(0.5)     \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2685f755",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5eed15d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "s0=np.zeros(128)\n",
    "sn=np.zeros(128)+255\n",
    "factor=0.5\n",
    "n_s=int((1/factor)*255)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ab13dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "states=np.zeros([n_s,128])\n",
    "for i in range(n_s):\n",
    "    states[i]=s0+factor*i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97606703",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS_VAL = 1e-16 #negligibleconstant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59f32c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of states and number of controls/action\n",
    "n_s=511\n",
    "n_a=3\n",
    "num_states=[n_s]\n",
    "num_controls=[n_a]\n",
    "\n",
    "num_obs=[n_s,3]\n",
    "num_factors=len(num_states)\n",
    "num_control_factors=len(num_controls)\n",
    "num_modalities = len(num_obs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c71c11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Blearned=obj_array(num_factors)\n",
    "Alearned=obj_array(num_modalities)\n",
    "\n",
    "Blearned[0]=np.zeros((n_s,n_s,n_a))\n",
    "\n",
    "Alearned[0]=np.zeros((n_s,n_s))\n",
    "Alearned[1]=np.zeros((3,n_s))\n",
    "\n",
    "#prior preferences in terms of observations\n",
    "C = obj_array_zeros(num_obs)\n",
    "C[1]=[-1,-1,1]\n",
    "\n",
    "#prior at t=0\n",
    "D = obj_array(num_factors)\n",
    "D[0]=np.zeros((num_states[0]))\n",
    "prior=D[0]\n",
    "\n",
    "#Sm_pars list\n",
    "sm_pars=[100]\n",
    "smpt=len(sm_pars)\n",
    "trials=1\n",
    "time_steps=1000\n",
    "T=4\n",
    "\n",
    "Alearned[0]=np.zeros((n_s,n_s))\n",
    "a_prior=obj_array(num_modalities)\n",
    "a_prior[0]=np.zeros((n_s,n_s))\n",
    "a_prior[1]=np.zeros((3,n_s))\n",
    "\n",
    "b_prior=np.zeros((n_s,n_s,n_a))\n",
    "numS=num_states[0]\n",
    "numA=num_controls[0]\n",
    "numO1=num_obs[0]\n",
    "numO2=num_obs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "939832fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes=np.zeros((smpt,trials,time_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b31ae570",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS_VAL = 1e-16 #negligibleconstant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3b054565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma 0\n",
      "Trial 0\n",
      "ts= 0\n",
      "reward= 0.0\n",
      "False\n",
      "tau= 1\n",
      "ts= 1\n",
      "reward= 0.0\n",
      "False\n",
      "tau= 2\n",
      "ts= 2\n",
      "reward= 0.0\n",
      "False\n",
      "tau= 3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10896/2990004391.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m                         \u001b[0mBlearned\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdirichlet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[1;31m#Replanning with new_evidence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[0mQactions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maction_dist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAlearned\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBlearned\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msm_pars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Desktop\\Cortical Labs\\PingPongOpenAIGym\\ai_agent_planner.py\u001b[0m in \u001b[0;36maction_dist\u001b[1;34m(A, B, C, T, sm_par)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m                     \u001b[0mG\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkl_div\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ_po\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC_po\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mjj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                         \u001b[1;32mfor\u001b[0m \u001b[0mkk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for sp in range(smpt):\n",
    "    print('Gamma',sp)\n",
    "    \n",
    "    for ii in range(trials):\n",
    "        print('Trial',ii)\n",
    "        epi=0\n",
    "        tau=0\n",
    "\n",
    "        #Learning Transition dynamics\n",
    "        #Dirichlet distribution (Priors)\n",
    "        b=b_prior+EPS_VAL #Hidden-states\n",
    "        a1=a_prior[0]+EPS_VAL #Mod-1\n",
    "        a2=a_prior[1]+EPS_VAL #Mod-2\n",
    "\n",
    "        Blearned=obj_array(num_factors)\n",
    "        Alearned=obj_array(num_modalities)\n",
    "        Blearned[0]=np.zeros((n_s,n_s,n_a))\n",
    "        Alearned[0]=np.zeros((n_s,n_s))\n",
    "        Alearned[1]=np.zeros((3,n_s))\n",
    "\n",
    "        for i in range(n_s):\n",
    "            Alearned[0][:,i]=dirichlet.mean(a1[:,i])\n",
    "            Alearned[1][:,i]=dirichlet.mean(a2[:,i])\n",
    "            for j in range(n_a):\n",
    "                Blearned[0][:,i,j]=dirichlet.mean(b[:,i,j])\n",
    "\n",
    "\n",
    "        #Planning using available A,B,C,T\n",
    "        Qactions=action_dist(Alearned,Blearned,C,T,sm_pars[sp])\n",
    "\n",
    "        for ts in range(time_steps):\n",
    "            print(\"ts=\",ts)\n",
    "            if(tau==0):\n",
    "                #New episode-start\n",
    "                prior=D[0]\n",
    "                global state\n",
    "                state=env.reset()\n",
    "                obs_idx=[state,0]\n",
    "\n",
    "            kingsmoves=[0,1,2]\n",
    "            #Perception\n",
    "            q_s=infer_state(prior,Alearned,obs_idx)\n",
    "\n",
    "            a1+=np.kron(q_s,onehot(obs_idx[0],numO1).reshape((-1,1)))\n",
    "            a2+=np.kron(q_s,onehot(obs_idx[1],numO2).reshape((-1,1)))\n",
    "\n",
    "            action_dist_qs=Qactions[tau,:,:].dot(q_s)\n",
    "            action=np.random.choice(kingsmoves,p=action_dist_qs)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            obs_idx[0]=state\n",
    "            obs_idx[1]=int(reward)\n",
    "            print(\"reward=\",reward)\n",
    "            print(done)\n",
    "\n",
    "            if(tau>1):\n",
    "                b[:,:,ac_mo]+=Qactions[tau,ac_mo,:].dot(q_s_mo)*np.kron(q_s_mo,q_s.reshape((-1,1)))\n",
    "\n",
    "            tau+=1\n",
    "            print(\"tau=\",tau)\n",
    "            #End of planning trial conditions\n",
    "            if(tau==T-1):\n",
    "                tau=0              \n",
    "                for i in range(numS):\n",
    "                    Alearned[0][:,i]=dirichlet.mean(a1[:,i])\n",
    "                    Alearned[1][:,i]=dirichlet.mean(a2[:,i])\n",
    "                    for j in range(numA):\n",
    "                        Blearned[0][:,i,j]=dirichlet.mean(b[:,i,j])\n",
    "                #Replanning with new_evidence\n",
    "                Qactions=action_dist(Alearned,Blearned,C,T,sm_pars[sp])\n",
    "\n",
    "            if(reward==1):\n",
    "                epi+=1\n",
    "                tau=0\n",
    "            #Updating episodes completed\n",
    "            episodes[sp,ii,ts]=epi\n",
    "\n",
    "#             Blearneddummy=np.zeros((numS,numS,numA))\n",
    "#             Alearneddummy=np.zeros((numO1,numA))\n",
    "#             Blearneddummy=np.array((Blearned[0]))\n",
    "#             Alearneddummy=np.array((Alearned[0]))\n",
    "#             for i in range(numS):\n",
    "#                 obsenoisedev_t_1[sp,ii,ts]+=kl_div(A_true[0][:,i],Alearneddummy[:,i])\n",
    "#                 for j in range(numA):\n",
    "#                     modeldeviation_t_1[sp,ii,ts]+=kl_div(Btrue_1[:,i,j],Blearneddummy[:,i,j])\n",
    "\n",
    "            #Setting-up priors for next time_step\n",
    "            prior=Blearned[0][:,:,action].dot(q_s)\n",
    "            q_s_mo=q_s\n",
    "            ac_mo=action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb79939",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4abd770",
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68750f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f80dbb81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_idx[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0589db10",
   "metadata": {},
   "outputs": [],
   "source": [
    "state=env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "11e2c380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([192,   0,   0,   0, 110,  38,   0,   7,  71,   1,  60,  59,   0,\n",
       "         0,   0,  62, 255,   0, 255, 253,   0,  22,   0,  24, 128,  32,\n",
       "         1,  86, 247,  86, 247,  86, 247, 134, 243, 245, 243, 240, 240,\n",
       "       242, 242,  32,  32,  64,  64,  64, 188,  65, 189,   0,  22, 109,\n",
       "        37,  37,  60,   0,   0,   0,   0, 109, 109,  37,  37, 192, 192,\n",
       "       192, 192,   1, 192, 202, 247, 202, 247, 202, 247, 202, 247,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,  54, 236, 242, 121, 240], dtype=uint8)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
