{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9188371c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aswin\\anaconda3\\lib\\site-packages\\seaborn\\rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(mpl.__version__) >= \"3.0\":\n",
      "C:\\Users\\aswin\\anaconda3\\lib\\site-packages\\setuptools\\_distutils\\version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n"
     ]
    }
   ],
   "source": [
    "from CustomPongEnv_v0_ram import Custom_Pong\n",
    "import numpy as np\n",
    "import random\n",
    "from math_helper_functions import pong_state_to_obs, normalise_A, normalise_B\n",
    "import pymdp\n",
    "from pymdp import utils\n",
    "import numpy as np\n",
    "from scipy.stats import dirichlet\n",
    "from pymdp.agent import Agent\n",
    "from ai_agent_planner_riskonly import action_dist\n",
    "from scipy.stats import entropy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "random.seed(10)\n",
    "np.random.seed(10)\n",
    "\n",
    "env = Custom_Pong()\n",
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n\n",
    "\n",
    "# (Hidden)Factors\n",
    "# Paddle (Hypothesis)\n",
    "s1_size = 42\n",
    "# Ball (Hypothesis)\n",
    "s2_size = 42\n",
    "\n",
    "num_states = [s1_size, s2_size]\n",
    "num_factors = len(num_states)\n",
    "\n",
    "# Rewards\n",
    "reward_modes = 3 #Max score-5 (assumption)\n",
    "\n",
    "# Controls\n",
    "s1_actions = ['Stay', 'Play-Up', 'Play-Down']\n",
    "s2_actions = ['Do nothing']\n",
    "\n",
    "num_controls = [len(s1_actions), len(s2_actions)]\n",
    "\n",
    "# Observations\n",
    "#Ball-x\n",
    "o1_obs_size = s1_size\n",
    "#Ball-y\n",
    "o2_obs_size = s1_size\n",
    "#Ball-vx\n",
    "o3_obs_size = 2\n",
    "#Ball-vy\n",
    "o4_obs_size = 2\n",
    "#Paddle-pos\n",
    "o5_obs_size = s1_size\n",
    "#Paddle-velocity\n",
    "o6_obs_size = 2\n",
    "#Reward (Shock, Chocolate, and Nothing)\n",
    "reward_obs_size = reward_modes\n",
    "\n",
    "num_obs = [o1_obs_size, o2_obs_size, o3_obs_size, o4_obs_size, o5_obs_size, o6_obs_size, reward_obs_size]\n",
    "num_modalities = len(num_obs)\n",
    "\n",
    "EPS_VAL = 1e-16 # Negligibleconstant\n",
    "\n",
    "# Likelhiood Dynamics\n",
    "A = utils.random_A_matrix(num_obs, num_states)*0 + EPS_VAL\n",
    "\n",
    "# Transisition dynamics\n",
    "# Initialised as random becuase the agent need to learn the dynamics\n",
    "\n",
    "B = utils.random_B_matrix(num_states, num_controls)*0 + EPS_VAL\n",
    "\n",
    "numS = 1\n",
    "for i in num_states:\n",
    "    numS *= i\n",
    "numA = 1\n",
    "for i in num_controls:\n",
    "    numA *= i\n",
    "\n",
    "A = normalise_A(A, num_states, num_modalities)\n",
    "B = normalise_B(B, num_states, num_controls)\n",
    "\n",
    "# Prior preferences for biasing the generative model to control behaviour\n",
    "\n",
    "# The preferences are set uniform for all the hidden-states except the reward function\n",
    "C = utils.obj_array_uniform(num_obs)\n",
    "\n",
    "# Highest for the high-score and lowest for the lowscore\n",
    "C_score = np.array([-5.8, 0 , 1])\n",
    "# Normalising the prior preference\n",
    "C[6] = pymdp.maths.softmax(1*C_score)\n",
    "\n",
    "D = utils.obj_array_uniform(num_states)\n",
    "\n",
    "A_naive = A\n",
    "B_naive = B\n",
    "\n",
    "# %time Q_pi = action_dist(A, B, C, T=5, sm_par=1)\n",
    "# %time qs = pymdp.inference.update_posterior_states(A, observation, prior = qs_prev)\n",
    "\n",
    "# Dynamic programming active inference simulations\n",
    "\n",
    "trials = 10\n",
    "episodes = 400\n",
    "planning_horizon = 3\n",
    "T = planning_horizon\n",
    "#Factor for resolution of obeservations\n",
    "factor = 5\n",
    "\n",
    "rally_length_raw = np.zeros((trials,episodes))\n",
    "e = []\n",
    "\n",
    "for trial in range(trials):\n",
    "    print(\"trial\", trial)\n",
    "    \n",
    "    EPS_VAL = 1e-16 #negligibleconstant\n",
    "    # Likelhiood Dynamics\n",
    "    A = A_naive\n",
    "\n",
    "    # Transisition dynamics\n",
    "    # Initialised as random becuase the agent need to learn the dynamics\n",
    "    B = B_naive\n",
    "\n",
    "    # Normalising A and B as probability distributions\n",
    "\n",
    "    A = normalise_A(A, num_states, num_modalities)\n",
    "    B = normalise_B(B, num_states, num_controls)\n",
    "\n",
    "    # Prior preferences for biasing the generative model to control behaviour is unchanging\n",
    "    \n",
    "    #Prior over hidden-states before a fresh trial\n",
    "    D = utils.obj_array_uniform(num_states)\n",
    "    \n",
    "    #Episodes inside a trial\n",
    "    for episode in range(episodes):\n",
    "        # print(\"episode\", episode)\n",
    "\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        t=0\n",
    "        #Planning for the upcoming episode\n",
    "        Q_pi = action_dist(A, B, C, T=planning_horizon, sm_par=1)\n",
    "        \n",
    "        while not done:\n",
    "            # env.render()\n",
    "            if(t==0):\n",
    "                qs = D\n",
    "                qs_prev = qs\n",
    "   \n",
    "            q_states_vec = np.kron(qs[0],qs[1])\n",
    "            \n",
    "            action = np.random.choice([0,1,2], size=None, replace=True, p=np.matmul(Q_pi[t],q_states_vec))\n",
    "            state, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "\n",
    "            observation = pong_state_to_obs(state, reward, factor)\n",
    "\n",
    "            qs = pymdp.inference.update_posterior_states(A, observation, prior = qs_prev)\n",
    "            \n",
    "            if(t == 0):\n",
    "                qs0 = qs\n",
    "\n",
    "            # Learning\n",
    "            A = pymdp.learning.update_obs_likelihood_dirichlet(A, A, observation, qs, lr=1.0, modalities='all')\n",
    "\n",
    "            actions = np.array([int(action),0])\n",
    "            B = pymdp.learning.update_state_likelihood_dirichlet(B, B, actions, qs, qs_prev, lr=1.0, factors='all')\n",
    "\n",
    "            e.append([np.sum(entropy(A[0])), np.sum(entropy(A[1])), np.sum(entropy(A[2])), np.sum(entropy(A[3])), np.sum(entropy(A[4])), np.sum(entropy(A[5])), np.sum(entropy(A[6])), np.sum(entropy(B[0])) , np.sum(entropy(B[1]))])\n",
    "            \n",
    "            if(t == 0):\n",
    "                D = pymdp.learning.update_state_prior_dirichlet(D, qs0, lr=1.0, factors='all')\n",
    "                D[0] = pymdp.maths.softmax(D[0])\n",
    "                D[1] = pymdp.maths.softmax(D[0])\n",
    "\n",
    "            t += 1\n",
    "            qs_prev = qs\n",
    "\n",
    "            if(t == T-1):\n",
    "                t = 0\n",
    "                #Normalising A and B\n",
    "                A = normalise_A(A, num_states, num_modalities)\n",
    "                B = normalise_B(B, num_states, num_controls)\n",
    "\n",
    "        rally_length_raw[trial][episode] = score+1\n",
    "\n",
    "    env.close()\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "x = np.array(e)\n",
    "x.shape\n",
    "\n",
    "fig, axs = plt.subplots(3,3,figsize=(20,10))\n",
    "fig.suptitle('entropy plots')\n",
    "\n",
    "axs[0,0].plot(x[:,0])\n",
    "axs[0,0].title.set_text('Ball x')\n",
    "axs[0,1].plot(x[:,1])\n",
    "axs[0,1].title.set_text('Ball y')\n",
    "axs[0,2].plot(x[:,2])\n",
    "axs[0,2].title.set_text('Ball vx')\n",
    "axs[1,0].plot(x[:,3])\n",
    "axs[1,0].title.set_text('Ball vy')\n",
    "axs[1,1].plot(x[:,4])\n",
    "axs[1,1].title.set_text('Paddle pos')\n",
    "axs[1,2].plot(x[:,5])\n",
    "axs[1,2].title.set_text('Paddle vel')\n",
    "axs[2,0].plot(x[:,6])\n",
    "axs[2,0].title.set_text('Score')\n",
    "\n",
    "axs[2,1].plot(x[:,7])\n",
    "axs[2,1].title.set_text('Transition B-1')\n",
    "axs[2,2].plot(x[:,8])\n",
    "axs[2,2].title.set_text('Transition B-2')\n",
    "plt.savefig('graph-3a.png', dpi=500, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "rally_length_1_raw = rally_length_raw[:, 0:int(episodes/4)]\n",
    "rally_length_2_raw = rally_length_raw[:, int(episodes/4):episodes]\n",
    "\n",
    "rally_length_1 = np.mean(rally_length_1_raw, axis=0)\n",
    "rally_length_2 = np.mean(rally_length_2_raw, axis=0)\n",
    "x = [rally_length_1, rally_length_2]\n",
    "\n",
    "#Plotting\n",
    "xy = [x[0], x[1]]\n",
    "\n",
    "plt.boxplot(xy, showmeans=True, positions=[1,2])\n",
    "plt.ylabel(\"Average rally length\")\n",
    "plt.xlabel(\"Act.Inf Agent(T=3)\")\n",
    "plt.title(\"Game play\")\n",
    "plt.savefig('graph-3.png', dpi=500, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935203f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
