{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cb155fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CustomPongEnv_v0_ram import Custom_Pong\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(10)\n",
    "np.random.seed(10)\n",
    "\n",
    "env = Custom_Pong()\n",
    "states=env.observation_space.shape[0]\n",
    "actions=env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c32fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_obs(state, reward):\n",
    "    \n",
    "    factor = 5\n",
    "    \n",
    "    ball_x = state[0]\n",
    "    o1_float = ball_x/factor\n",
    "    o1 = int(o1_float)\n",
    "\n",
    "    ball_y = state[1] \n",
    "    o2_float = ball_y/factor\n",
    "    o2 = int(o2_float)\n",
    "\n",
    "    ball_vx = state[2]\n",
    "    o3_float = ball_vx\n",
    "    o3_i = int(o3_float)\n",
    "    o3 = 0 if o3_i==2 else 1 \n",
    "\n",
    "\n",
    "    ball_vy = state[3]\n",
    "    o4_float = ball_vx\n",
    "    o4_i = int(o4_float)\n",
    "    o4 = 0 if o4_i==2 else 1 \n",
    "\n",
    "    paddle_pos = state[4]\n",
    "    o5_float = paddle_pos/factor\n",
    "    o5 = int(o5_float)\n",
    "\n",
    "    paddle_vel = state[5]\n",
    "    o6_float = paddle_vel\n",
    "    o6_i = int(o6_float)\n",
    "    o6 = 0 if o6_i==4 else 1 \n",
    "    \n",
    "    if(reward == -1):\n",
    "        o7 = 0\n",
    "    elif(reward == 1):\n",
    "        o7 = 2\n",
    "    else:\n",
    "        o7 = 1\n",
    "    \n",
    "    observation = [o1, o2, o3, o4, o5, o6, o7]\n",
    "    \n",
    "    return(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f9acef",
   "metadata": {},
   "source": [
    "# Active Inference\n",
    "## Setting up states and factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3de56ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda/2019.03-Python3.7-gcc5/lib/python3.7/site-packages/pandas/util/testing.py:20: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from pandas._libs import testing as _testing\n",
      "/usr/local/anaconda/2019.03-Python3.7-gcc5/lib/python3.7/site-packages/scipy/special/orthogonal.py:81: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from numpy import (exp, inf, pi, sqrt, floor, sin, cos, around, int,\n",
      "/usr/local/anaconda/2019.03-Python3.7-gcc5/lib/python3.7/site-packages/scipy/linalg/__init__.py:212: DeprecationWarning: The module numpy.dual is deprecated.  Instead of using dual, use the functions directly from numpy or scipy.\n",
      "  from numpy.dual import register_func\n",
      "/usr/local/anaconda/2019.03-Python3.7-gcc5/lib/python3.7/site-packages/scipy/sparse/sputils.py:16: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  supported_dtypes = [np.typeDict[x] for x in supported_dtypes]\n"
     ]
    }
   ],
   "source": [
    "import pymdp\n",
    "from pymdp import utils\n",
    "import numpy as np\n",
    "from scipy.stats import dirichlet\n",
    "from pymdp.agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19288871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Hidden)Factors\n",
    "s1_size = 42\n",
    "\n",
    "num_states = [s1_size]\n",
    "num_factors = len(num_states)\n",
    "\n",
    "# Rewards\n",
    "reward_modes = 3 #Max score-5 (assumption)\n",
    "\n",
    "# Controls\n",
    "s1_actions = ['Stay', 'Play-Up', 'Play-Down']\n",
    "\n",
    "num_controls = [len(s1_actions)]\n",
    "\n",
    "# Observations\n",
    "o1_obs_size = 42\n",
    "o2_obs_size = 42\n",
    "o3_obs_size = 2\n",
    "o4_obs_size = 2\n",
    "o5_obs_size = 42\n",
    "o6_obs_size = 2\n",
    "reward_obs_size = reward_modes\n",
    "\n",
    "num_obs = [o1_obs_size, o2_obs_size, o3_obs_size, o4_obs_size, o5_obs_size, o6_obs_size, reward_obs_size]\n",
    "num_modalities = len(num_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28a21b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS_VAL = 1e-16 # Negligibleconstant\n",
    "\n",
    "# Likelhiood Dynamics\n",
    "a = utils.random_A_matrix(num_obs, num_states) * 0 + EPS_VAL\n",
    "A = utils.random_A_matrix(num_obs, num_states)\n",
    "\n",
    "# Transisition dynamics\n",
    "# Initialised as random becuase the agent need to learn the dynamics\n",
    "\n",
    "B = utils.random_B_matrix(num_states, num_controls)\n",
    "b = utils.random_B_matrix(num_states, num_controls) * 0 + EPS_VAL\n",
    "B[0].shape\n",
    "\n",
    "# Normalising A and B as probability distributions\n",
    "\n",
    "numS = num_states[0]\n",
    "numA = num_controls[0]\n",
    "\n",
    "for i in range(numS):\n",
    "    A[0][:,i] = dirichlet.mean(a[0][:,i])\n",
    "    A[1][:,i] = dirichlet.mean(a[1][:,i])\n",
    "    A[2][:,i] = dirichlet.mean(a[2][:,i])\n",
    "    A[3][:,i] = dirichlet.mean(a[3][:,i])\n",
    "    A[4][:,i] = dirichlet.mean(a[4][:,i])\n",
    "    A[5][:,i] = dirichlet.mean(a[5][:,i])\n",
    "    A[6][:,i] = dirichlet.mean(a[6][:,i])\n",
    "    for j in range(numA):\n",
    "        B[0][:,i,j]=dirichlet.mean(b[0][:,i,j])\n",
    "\n",
    "# Prior preferences for biasing the generative model to control behaviour\n",
    "\n",
    "# The preferences are set uniform for all the hidden-states except the reward function\n",
    "C = utils.obj_array_uniform(num_obs)\n",
    "\n",
    "# Highest for the high-score and lowest for the lowscore\n",
    "C_score = np.array([-5.8, 0 , 1])\n",
    "# Normalising the prior preference\n",
    "C[6] = pymdp.maths.softmax(1*C_score)\n",
    "C[6]\n",
    "\n",
    "D = utils.obj_array_uniform([s1_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04811d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic programming active inference planner\n",
    "from ai_agent_planner import action_dist\n",
    "\n",
    "trials = 10\n",
    "episodes = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ec04224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planning horizon 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ac1ceba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trials 0\n",
      "trials 1\n",
      "trials 2\n",
      "trials 3\n",
      "trials 4\n",
      "trials 5\n",
      "trials 6\n",
      "trials 7\n",
      "trials 8\n",
      "trials 9\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "CPU times: user 16h 47min 53s, sys: 3min 23s, total: 16h 51min 17s\n",
      "Wall time: 16h 47min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "planning_horizon = 20\n",
    "T = planning_horizon\n",
    "\n",
    "rally_length_raw_20 = np.zeros((trials,episodes))\n",
    "\n",
    "for trial in range(trials):\n",
    "    print(\"trials\", trial)\n",
    "    \n",
    "    EPS_VAL = 1e-16 #negligibleconstant\n",
    "    # Likelhiood Dynamics\n",
    "    a = utils.random_A_matrix(num_obs, num_states) * 0 + EPS_VAL\n",
    "    A = utils.random_A_matrix(num_obs, num_states)\n",
    "\n",
    "    # Transisition dynamics\n",
    "    # Initialised as random becuase the agent need to learn the dynamics\n",
    "\n",
    "    B = utils.random_B_matrix(num_states, num_controls)\n",
    "    b = utils.random_B_matrix(num_states, num_controls) * 0 + EPS_VAL\n",
    "    B[0].shape\n",
    "\n",
    "    # Normalising A and B as probability distributions\n",
    "\n",
    "    numS = num_states[0]\n",
    "    numA = num_controls[0]\n",
    "\n",
    "    for i in range(numS):\n",
    "        A[0][:,i] = dirichlet.mean(a[0][:,i])\n",
    "        A[1][:,i] = dirichlet.mean(a[1][:,i])\n",
    "        A[2][:,i] = dirichlet.mean(a[2][:,i])\n",
    "        A[3][:,i] = dirichlet.mean(a[3][:,i])\n",
    "        A[4][:,i] = dirichlet.mean(a[4][:,i])\n",
    "        A[5][:,i] = dirichlet.mean(a[5][:,i])\n",
    "        A[6][:,i] = dirichlet.mean(a[6][:,i])\n",
    "        for j in range(numA):\n",
    "            B[0][:,i,j]=dirichlet.mean(b[0][:,i,j])\n",
    "\n",
    "    # Prior preferences for biasing the generative model to control behaviour\n",
    "\n",
    "    # The preferences are set uniform for all the hidden-states except the reward function\n",
    "    C = utils.obj_array_uniform(num_obs)\n",
    "\n",
    "    # Highest for the high-score and lowest for the lowscore\n",
    "    C_score = np.array([-5.8, 0 , 1])\n",
    "    # Normalising the prior preference\n",
    "    C[6] = pymdp.maths.softmax(1*C_score)\n",
    "    C[6]\n",
    "\n",
    "    D = utils.obj_array_uniform([s1_size])\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        # print(\"episode:\", episode)\n",
    "\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        t=0\n",
    "        while not done:\n",
    "            # env.render()\n",
    "            # print(t)\n",
    "            if(t==0):\n",
    "                qs = D\n",
    "                qs_prev = qs\n",
    "                Q_pi = action_dist(A[6], B, C[6], T=planning_horizon, sm_par=1)\n",
    "            \n",
    "            action = np.random.choice([0,1,2], size=None, replace=True, p=np.matmul(Q_pi[t],qs[0]))\n",
    "\n",
    "            state, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "\n",
    "            observation = state_to_obs(state, reward)\n",
    "\n",
    "            qs = pymdp.inference.update_posterior_states(A, observation, prior = D)\n",
    "            if(t == 0):\n",
    "                qs0 = qs\n",
    "\n",
    "            pA = A\n",
    "            pB = B\n",
    "            pD = D\n",
    "\n",
    "            # Learning\n",
    "            A = pymdp.learning.update_obs_likelihood_dirichlet(pA, A, observation, qs, lr=1.0, modalities='all')\n",
    "\n",
    "            actions = np.array([int(action)])\n",
    "            B = pymdp.learning.update_state_likelihood_dirichlet(pB, B, actions, qs, qs_prev, lr=1.0, factors='all')\n",
    "\n",
    "            if(t == 0):\n",
    "                D = pymdp.learning.update_state_prior_dirichlet(pD, qs0, lr=1.0, factors='all')\n",
    "                D[0] = pymdp.maths.softmax(D[0])\n",
    "\n",
    "            t += 1\n",
    "            qs_prev = qs\n",
    "\n",
    "            if(t == T-1):\n",
    "                t = 0\n",
    "                #Normalising A and B\n",
    "                for i in range(numS):\n",
    "                    A[0][:,i] = dirichlet.mean(A[0][:,i])\n",
    "                    A[1][:,i] = dirichlet.mean(A[1][:,i])\n",
    "                    A[2][:,i] = dirichlet.mean(A[2][:,i])\n",
    "                    A[3][:,i] = dirichlet.mean(A[3][:,i])\n",
    "                    A[4][:,i] = dirichlet.mean(A[4][:,i])\n",
    "                    A[5][:,i] = dirichlet.mean(A[5][:,i])\n",
    "                    A[6][:,i] = dirichlet.mean(A[6][:,i])\n",
    "                    for j in range(numA):\n",
    "                        B[0][:,i,j]=dirichlet.mean(B[0][:,i,j])\n",
    "\n",
    "        rally_length_raw_20[trial][episode] = score+1\n",
    "\n",
    "    env.close()\n",
    "    \n",
    "with open('rally_length_T20.npy', 'wb') as file:\n",
    "    np.save(file, rally_length_raw_20)\n",
    "\n",
    "# Random agent\n",
    "\n",
    "rally_length_raw_r = np.zeros((trials,episodes))\n",
    "\n",
    "for trial in range(trials):\n",
    "    print(trial)\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        while not done:\n",
    "            #env.render()\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "            state, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "\n",
    "        rally_length_raw_r[trial][episode] = score+1\n",
    "\n",
    "    env.close()\n",
    "    \n",
    "with open('rally_length_r.npy', 'wb') as file:\n",
    "    np.save(file, rally_length_raw_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f07690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
