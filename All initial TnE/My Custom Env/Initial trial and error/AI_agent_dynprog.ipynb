{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd76e633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CustomPongEnv_v0_ram import Custom_Pong\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(10)\n",
    "np.random.seed(10)\n",
    "\n",
    "env = Custom_Pong()\n",
    "states=env.observation_space.shape[0]\n",
    "actions=env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f045382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_obs(state, streward):\n",
    "    \n",
    "    factor = 5\n",
    "    \n",
    "    ball_x = state[0]\n",
    "    o1_float = ball_x/factor\n",
    "    o1 = int(o1_float)\n",
    "\n",
    "    ball_y = state[1] \n",
    "    o2_float = ball_y/factor\n",
    "    o2 = int(o2_float)\n",
    "\n",
    "    ball_vx = state[2]\n",
    "    o3_float = ball_vx\n",
    "    o3_i = int(o3_float)\n",
    "    o3 = 0 if o3_i==2 else 1 \n",
    "\n",
    "\n",
    "    ball_vy = state[3]\n",
    "    o4_float = ball_vx\n",
    "    o4_i = int(o4_float)\n",
    "    o4 = 0 if o4_i==2 else 1 \n",
    "\n",
    "    paddle_pos = state[4]\n",
    "    o5_float = paddle_pos/factor\n",
    "    o5 = int(o5_float)\n",
    "\n",
    "    paddle_vel = state[5]\n",
    "    o6_float = paddle_vel\n",
    "    o6_i = int(o6_float)\n",
    "    o6 = 0 if o6_i==4 else 1 \n",
    "    \n",
    "    if(reward == -1):\n",
    "        o7 = 0\n",
    "    elif(reward == 1):\n",
    "        o7 = 2\n",
    "    else:\n",
    "        o7 = 1\n",
    "    \n",
    "    observation = [o1, o2, o3, o4, o5, o6, o7]\n",
    "    \n",
    "    return(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17437183",
   "metadata": {},
   "source": [
    "# Active Inference\n",
    "## Setting up states and factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84126d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aswin\\anaconda3\\lib\\site-packages\\seaborn\\rcmod.py:82: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(mpl.__version__) >= \"3.0\":\n",
      "C:\\Users\\aswin\\anaconda3\\lib\\site-packages\\setuptools\\_distutils\\version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n"
     ]
    }
   ],
   "source": [
    "import pymdp\n",
    "from pymdp import utils\n",
    "import numpy as np\n",
    "from scipy.stats import dirichlet\n",
    "from pymdp.agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a80b98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Hidden)Factors\n",
    "s1_size = 42\n",
    "\n",
    "num_states = [s1_size]\n",
    "num_factors = len(num_states)\n",
    "\n",
    "# Rewards\n",
    "reward_modes = 3 #Max score-5 (assumption)\n",
    "\n",
    "# Controls\n",
    "s1_actions = ['Stay', 'Play-Up', 'Play-Down']\n",
    "\n",
    "num_controls = [len(s1_actions)]\n",
    "\n",
    "# Observations\n",
    "o1_obs_size = 42\n",
    "o2_obs_size = 42\n",
    "o3_obs_size = 2\n",
    "o4_obs_size = 2\n",
    "o5_obs_size = 42\n",
    "o6_obs_size = 2\n",
    "reward_obs_size = reward_modes\n",
    "\n",
    "num_obs = [o1_obs_size, o2_obs_size, o3_obs_size, o4_obs_size, o5_obs_size, o6_obs_size, reward_obs_size]\n",
    "num_modalities = len(num_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6fa66f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS_VAL = 1e-16 # Negligibleconstant\n",
    "\n",
    "# Likelhiood Dynamics\n",
    "a = utils.random_A_matrix(num_obs, num_states) * 0 + EPS_VAL\n",
    "A = utils.random_A_matrix(num_obs, num_states)\n",
    "\n",
    "# Transisition dynamics\n",
    "# Initialised as random becuase the agent need to learn the dynamics\n",
    "\n",
    "B = utils.random_B_matrix(num_states, num_controls)\n",
    "b = utils.random_B_matrix(num_states, num_controls) * 0 + EPS_VAL\n",
    "B[0].shape\n",
    "\n",
    "# Normalising A and B as probability distributions\n",
    "\n",
    "numS = num_states[0]\n",
    "numA = num_controls[0]\n",
    "\n",
    "for i in range(numS):\n",
    "    A[0][:,i] = dirichlet.mean(a[0][:,i])\n",
    "    A[1][:,i] = dirichlet.mean(a[1][:,i])\n",
    "    A[2][:,i] = dirichlet.mean(a[2][:,i])\n",
    "    A[3][:,i] = dirichlet.mean(a[3][:,i])\n",
    "    A[4][:,i] = dirichlet.mean(a[4][:,i])\n",
    "    A[5][:,i] = dirichlet.mean(a[5][:,i])\n",
    "    A[6][:,i] = dirichlet.mean(a[6][:,i])\n",
    "    for j in range(numA):\n",
    "        B[0][:,i,j]=dirichlet.mean(b[0][:,i,j])\n",
    "\n",
    "# Prior preferences for biasing the generative model to control behaviour\n",
    "\n",
    "# The preferences are set uniform for all the hidden-states except the reward function\n",
    "C = utils.obj_array_uniform(num_obs)\n",
    "\n",
    "# Highest for the high-score and lowest for the lowscore\n",
    "C_score = np.array([-5.8, 0 , 1])\n",
    "# Normalising the prior preference\n",
    "C[6] = pymdp.maths.softmax(1*C_score)\n",
    "C[6]\n",
    "\n",
    "D = utils.obj_array_uniform([s1_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df26ae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic programming active inference planner\n",
    "from ai_agent_planner import action_dist\n",
    "\n",
    "trials = 4\n",
    "episodes = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26af9160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AI Agent-1 Planning horizon=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a44f12b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial 0\n",
      "trial 1\n",
      "trial 2\n",
      "trial 3\n",
      "CPU times: total: 1h 10min 10s\n",
      "Wall time: 1h 10min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "planning_horizon = 5\n",
    "T = planning_horizon\n",
    "\n",
    "rally_length_raw = np.zeros((trials,episodes))\n",
    "\n",
    "for trial in range(trials):\n",
    "    print(\"trial\", trial)\n",
    "    \n",
    "    EPS_VAL = 1e-16 #negligibleconstant\n",
    "    # Likelhiood Dynamics\n",
    "    a = utils.random_A_matrix(num_obs, num_states) * 0 + EPS_VAL\n",
    "    A = utils.random_A_matrix(num_obs, num_states)\n",
    "\n",
    "    # Transisition dynamics\n",
    "    # Initialised as random becuase the agent need to learn the dynamics\n",
    "\n",
    "    B = utils.random_B_matrix(num_states, num_controls)\n",
    "    b = utils.random_B_matrix(num_states, num_controls) * 0 + EPS_VAL\n",
    "    B[0].shape\n",
    "\n",
    "    # Normalising A and B as probability distributions\n",
    "\n",
    "    numS = num_states[0]\n",
    "    numA = num_controls[0]\n",
    "\n",
    "    for i in range(numS):\n",
    "        A[0][:,i] = dirichlet.mean(a[0][:,i])\n",
    "        A[1][:,i] = dirichlet.mean(a[1][:,i])\n",
    "        A[2][:,i] = dirichlet.mean(a[2][:,i])\n",
    "        A[3][:,i] = dirichlet.mean(a[3][:,i])\n",
    "        A[4][:,i] = dirichlet.mean(a[4][:,i])\n",
    "        A[5][:,i] = dirichlet.mean(a[5][:,i])\n",
    "        A[6][:,i] = dirichlet.mean(a[6][:,i])\n",
    "        for j in range(numA):\n",
    "            B[0][:,i,j]=dirichlet.mean(b[0][:,i,j])\n",
    "\n",
    "    # Prior preferences for biasing the generative model to control behaviour\n",
    "\n",
    "    # The preferences are set uniform for all the hidden-states except the reward function\n",
    "    C = utils.obj_array_uniform(num_obs)\n",
    "\n",
    "    # Highest for the high-score and lowest for the lowscore\n",
    "    C_score = np.array([-5.8, 0 , 1])\n",
    "    # Normalising the prior preference\n",
    "    C[6] = pymdp.maths.softmax(1*C_score)\n",
    "    C[6]\n",
    "\n",
    "    D = utils.obj_array_uniform([s1_size])\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        # print(\"episode\", episode)\n",
    "\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        t=0\n",
    "        while not done:\n",
    "            # env.render()\n",
    "            if(t==0):\n",
    "                qs = D\n",
    "                qs_prev = qs\n",
    "\n",
    "            Q_pi = action_dist(A[6], B, C[6], T=planning_horizon, sm_par=1)\n",
    "            action = np.random.choice([0,1,2], size=None, replace=True, p=np.matmul(Q_pi[t],qs[0]))\n",
    "\n",
    "            state, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "\n",
    "            observation = state_to_obs(state, reward)\n",
    "\n",
    "            qs = pymdp.inference.update_posterior_states(A, observation, prior = D)\n",
    "            if(t == 0):\n",
    "                qs0 = qs\n",
    "\n",
    "            pA = A\n",
    "            pB = B\n",
    "            pD = D\n",
    "\n",
    "            # Learning\n",
    "            A = pymdp.learning.update_obs_likelihood_dirichlet(pA, A, observation, qs, lr=1.0, modalities='all')\n",
    "\n",
    "            actions = np.array([int(action)])\n",
    "            B = pymdp.learning.update_state_likelihood_dirichlet(pB, B, actions, qs, qs_prev, lr=1.0, factors='all')\n",
    "\n",
    "            if(t == 0):\n",
    "                D = pymdp.learning.update_state_prior_dirichlet(pD, qs0, lr=1.0, factors='all')\n",
    "                D[0] = pymdp.maths.softmax(D[0])\n",
    "\n",
    "            t += 1\n",
    "            qs_prev = qs\n",
    "\n",
    "            if(t == T-1):\n",
    "                t = 0\n",
    "                #Normalising A and B\n",
    "                for i in range(numS):\n",
    "                    A[0][:,i] = dirichlet.mean(A[0][:,i])\n",
    "                    A[1][:,i] = dirichlet.mean(A[1][:,i])\n",
    "                    A[2][:,i] = dirichlet.mean(A[2][:,i])\n",
    "                    A[3][:,i] = dirichlet.mean(A[3][:,i])\n",
    "                    A[4][:,i] = dirichlet.mean(A[4][:,i])\n",
    "                    A[5][:,i] = dirichlet.mean(A[5][:,i])\n",
    "                    A[6][:,i] = dirichlet.mean(A[6][:,i])\n",
    "                    for j in range(numA):\n",
    "                        B[0][:,i,j]=dirichlet.mean(B[0][:,i,j])\n",
    "\n",
    "        rally_length_raw[trial][episode] = score+1\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4329c20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rally_length_1_raw = rally_length_raw[:, 0:int(episodes/4)]\n",
    "rally_length_2_raw = rally_length_raw[:, int(episodes/4):episodes]\n",
    "\n",
    "rally_length_1 = np.mean(rally_length_1_raw, axis=0)\n",
    "rally_length_2 = np.mean(rally_length_2_raw, axis=0)\n",
    "x = [rally_length_1, rally_length_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24bbc8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planning horizon 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3d08d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trials 0\n",
      "trials 1\n",
      "trials 2\n",
      "trials 3\n",
      "CPU times: total: 6h 47min 21s\n",
      "Wall time: 6h 47min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "planning_horizon = 20\n",
    "T = planning_horizon\n",
    "\n",
    "rally_length_raw_20 = np.zeros((trials,episodes))\n",
    "\n",
    "for trial in range(trials):\n",
    "    print(\"trials\", trial)\n",
    "    \n",
    "    EPS_VAL = 1e-16 #negligibleconstant\n",
    "    # Likelhiood Dynamics\n",
    "    a = utils.random_A_matrix(num_obs, num_states) * 0 + EPS_VAL\n",
    "    A = utils.random_A_matrix(num_obs, num_states)\n",
    "\n",
    "    # Transisition dynamics\n",
    "    # Initialised as random becuase the agent need to learn the dynamics\n",
    "\n",
    "    B = utils.random_B_matrix(num_states, num_controls)\n",
    "    b = utils.random_B_matrix(num_states, num_controls) * 0 + EPS_VAL\n",
    "    B[0].shape\n",
    "\n",
    "    # Normalising A and B as probability distributions\n",
    "\n",
    "    numS = num_states[0]\n",
    "    numA = num_controls[0]\n",
    "\n",
    "    for i in range(numS):\n",
    "        A[0][:,i] = dirichlet.mean(a[0][:,i])\n",
    "        A[1][:,i] = dirichlet.mean(a[1][:,i])\n",
    "        A[2][:,i] = dirichlet.mean(a[2][:,i])\n",
    "        A[3][:,i] = dirichlet.mean(a[3][:,i])\n",
    "        A[4][:,i] = dirichlet.mean(a[4][:,i])\n",
    "        A[5][:,i] = dirichlet.mean(a[5][:,i])\n",
    "        A[6][:,i] = dirichlet.mean(a[6][:,i])\n",
    "        for j in range(numA):\n",
    "            B[0][:,i,j]=dirichlet.mean(b[0][:,i,j])\n",
    "\n",
    "    # Prior preferences for biasing the generative model to control behaviour\n",
    "\n",
    "    # The preferences are set uniform for all the hidden-states except the reward function\n",
    "    C = utils.obj_array_uniform(num_obs)\n",
    "\n",
    "    # Highest for the high-score and lowest for the lowscore\n",
    "    C_score = np.array([-5.8, 0 , 1])\n",
    "    # Normalising the prior preference\n",
    "    C[6] = pymdp.maths.softmax(1*C_score)\n",
    "    C[6]\n",
    "\n",
    "    D = utils.obj_array_uniform([s1_size])\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        # print(\"episode:\", episode)\n",
    "\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        t=0\n",
    "        while not done:\n",
    "            # env.render()\n",
    "            if(t==0):\n",
    "                qs = D\n",
    "                qs_prev = qs\n",
    "\n",
    "            Q_pi = action_dist(A[6], B, C[6], T=planning_horizon, sm_par=1)\n",
    "            action = np.random.choice([0,1,2], size=None, replace=True, p=np.matmul(Q_pi[t],qs[0]))\n",
    "\n",
    "            state, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "\n",
    "            observation = state_to_obs(state, reward)\n",
    "\n",
    "            qs = pymdp.inference.update_posterior_states(A, observation, prior = D)\n",
    "            if(t == 0):\n",
    "                qs0 = qs\n",
    "\n",
    "            pA = A\n",
    "            pB = B\n",
    "            pD = D\n",
    "\n",
    "            # Learning\n",
    "            A = pymdp.learning.update_obs_likelihood_dirichlet(pA, A, observation, qs, lr=1.0, modalities='all')\n",
    "\n",
    "            actions = np.array([int(action)])\n",
    "            B = pymdp.learning.update_state_likelihood_dirichlet(pB, B, actions, qs, qs_prev, lr=1.0, factors='all')\n",
    "\n",
    "            if(t == 0):\n",
    "                D = pymdp.learning.update_state_prior_dirichlet(pD, qs0, lr=1.0, factors='all')\n",
    "                D[0] = pymdp.maths.softmax(D[0])\n",
    "\n",
    "            t += 1\n",
    "            qs_prev = qs\n",
    "\n",
    "            if(t == T-1):\n",
    "                t = 0\n",
    "                #Normalising A and B\n",
    "                for i in range(numS):\n",
    "                    A[0][:,i] = dirichlet.mean(A[0][:,i])\n",
    "                    A[1][:,i] = dirichlet.mean(A[1][:,i])\n",
    "                    A[2][:,i] = dirichlet.mean(A[2][:,i])\n",
    "                    A[3][:,i] = dirichlet.mean(A[3][:,i])\n",
    "                    A[4][:,i] = dirichlet.mean(A[4][:,i])\n",
    "                    A[5][:,i] = dirichlet.mean(A[5][:,i])\n",
    "                    A[6][:,i] = dirichlet.mean(A[6][:,i])\n",
    "                    for j in range(numA):\n",
    "                        B[0][:,i,j]=dirichlet.mean(B[0][:,i,j])\n",
    "\n",
    "        rally_length_raw_20[trial][episode] = score+1\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02740d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "rally_length_1_raw_20 = rally_length_raw_20[:, 0:int(episodes/4)]\n",
    "rally_length_2_raw_20 = rally_length_raw_20[:, int(episodes/4):episodes]\n",
    "\n",
    "rally_length_1_20 = np.mean(rally_length_1_raw_20, axis=0)\n",
    "rally_length_2_20 = np.mean(rally_length_2_raw_20, axis=0)\n",
    "\n",
    "x_20 = [rally_length_1_20, rally_length_2_20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ccf56b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4ecd8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "rally_length_raw_r = np.zeros((trials,episodes))\n",
    "\n",
    "for trial in range(trials):\n",
    "    print(trial)\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        while not done:\n",
    "            #env.render()\n",
    "            action = env.action_space.sample()\n",
    "\n",
    "            state, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "\n",
    "        rally_length_raw_r[trial][episode] = score+1\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "045f1abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rally_length_1_raw_r = rally_length_raw_r[:, 0:int(episodes/4)]\n",
    "rally_length_2_raw_r = rally_length_raw_r[:, int(episodes/4):episodes]\n",
    "\n",
    "rally_length_1_r = np.mean(rally_length_1_raw_r, axis=0)\n",
    "rally_length_2_r = np.mean(rally_length_2_raw_r, axis=0)\n",
    "y = [rally_length_1_r, rally_length_2_r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2c6c730",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a8ca911",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = [x[0], x[1], x_20[0], x_20[1], y[0], y[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa91ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e76fe7b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mboxplot(xy, showmeans\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, positions\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m8\u001b[39m])\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage rally length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAct.Inf Agent(T=5), Act.Inf Agent(T=20), Random Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.boxplot(xy, showmeans=True, positions=[1,2,4,5,7,8])\n",
    "plt.ylabel(\"Average rally length\")\n",
    "plt.xlabel(\"Act.Inf Agent(T=5), Act.Inf Agent(T=20), Random Agent\")\n",
    "plt.title(\"Game play\")\n",
    "plt.savefig('graph.png', dpi=500, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
